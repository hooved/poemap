{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4568c2-1b19-4038-a121-9273b86a4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from:\n",
    "https://github.com/milesial/Pytorch-UNet\n",
    "https://github.com/tinygrad/tinygrad/examples/stable_diffusion.py\n",
    "https://docs.tinygrad.org/mnist/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994a7f9-78fc-488f-853e-57392a1f0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, TinyJit, nn\n",
    "from tinygrad.nn import Conv2d, ConvTranspose2d, BatchNorm2d\n",
    "from tinygrad.dtype import dtypes\n",
    "from tinygrad.nn.state import safe_load, safe_save, get_state_dict, load_state_dict\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837a9d3-b471-453b-b13b-cc92f03de798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleconv(in_chan, out_chan):\n",
    "    return [Conv2d(in_chan, out_chan, kernel_size=3, padding=1), BatchNorm2d(out_chan), Tensor.relu,\n",
    "        Conv2d(out_chan, out_chan, kernel_size=3, padding=1), BatchNorm2d(out_chan), Tensor.relu]\n",
    "\n",
    "class UNet:\n",
    "    def __init__(self):\n",
    "        self.save_intermediates = [\n",
    "            doubleconv(3, 64), \n",
    "            [Tensor.max_pool2d, *doubleconv(64, 128)],\n",
    "        ]\n",
    "        self.middle = [\n",
    "            Tensor.max_pool2d, *doubleconv(128, 256),\n",
    "            ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "        ]\n",
    "        self.consume_intermediates = [\n",
    "            [*doubleconv(256, 128), ConvTranspose2d(128, 64, kernel_size=2, stride=2)],\n",
    "            [*doubleconv(128, 64), Conv2d(64, 2, kernel_size=1)],\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        intermediates = []\n",
    "        for b in self.save_intermediates:\n",
    "            for bb in b:\n",
    "                x = bb(x)\n",
    "            intermediates.append(x)\n",
    "        for bb in self.middle:\n",
    "            x = bb(x)\n",
    "        for b in self.consume_intermediates:\n",
    "            x = intermediates.pop().cat(x, dim=1)\n",
    "            for bb in b:\n",
    "                x = bb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64663adb-435b-491a-ba16-e27389c0ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMaskPair:\n",
    "    def __init__(self, image_path, mask_path):\n",
    "        self.image_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "\n",
    "    def load_image(self):\n",
    "        return np.load(self.image_path)['data']\n",
    "\n",
    "    def load_mask(self):\n",
    "        return np.load(self.mask_path)['data']\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, image_dir, mask_dir, patch_size=(64, 64), normalize=True, \n",
    "                 flip_prob=0.5, rotate_prob=0.5, noise_prob=0,):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.normalize = normalize\n",
    "        self.flip_prob = flip_prob\n",
    "        self.rotate_prob = rotate_prob\n",
    "        self.noise_prob = noise_prob\n",
    "        self.image_mask_pairs = self.get_image_mask_pairs()\n",
    "\n",
    "    def get_image_mask_pairs(self):\n",
    "        ret = []\n",
    "        for subdir in os.listdir(self.image_dir):\n",
    "            for file in os.listdir(os.path.join(self.image_dir, subdir)):\n",
    "                im_file = os.path.join(self.image_dir, subdir, file)\n",
    "                mask_file = os.path.join(self.mask_dir, subdir + \".npz\")\n",
    "                ret.append(ImageMaskPair(im_file, mask_file))\n",
    "\n",
    "        ret = sorted(ret, key = lambda x: x.mask_path)\n",
    "        return ret\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # Randomly distribute samples across images\n",
    "        shares = np.random.dirichlet(np.ones(len(self.image_mask_pairs)), size=1)[0]\n",
    "        result = np.round(shares * batch_size).astype(int)\n",
    "        # Adjust to ensure sum is exactly batch_size\n",
    "        diff = batch_size - result.sum()\n",
    "        result[np.argmax(result)] += diff\n",
    "\n",
    "        image_patches, mask_patches = [], []\n",
    "        mask_cache = {}\n",
    "        for i, num_samples in enumerate(result):\n",
    "            imp = self.image_mask_pairs[i]\n",
    "            if mask_cache.get(imp.mask_path) is None:\n",
    "                # We sorted image_mask_pairs by mask, so we don't need to cache previous masks\n",
    "                mask_cache = {imp.mask_path: imp.load_mask()}\n",
    "            mask = mask_cache[imp.mask_path]\n",
    "            image = imp.load_image()\n",
    "            image = self._normalize(image) if self.normalize else image\n",
    "            for _ in range(num_samples):\n",
    "                ip, mp = self._random_crop(image, mask)\n",
    "                ip, mp = self._apply_augmentations(ip, mp)\n",
    "                image_patches.append(ip)\n",
    "                mask_patches.append(mp)\n",
    "        image_patches = Tensor(image_patches).permute(0,3,1,2)\n",
    "        return image_patches, Tensor(mask_patches)\n",
    "\n",
    "    def _apply_augmentations(self, image, mask):\n",
    "        if random.random() < self.flip_prob:\n",
    "            image, mask = self._random_flip(image, mask)\n",
    "        if random.random() < self.rotate_prob:\n",
    "            image, mask = self._random_rotate(image, mask)\n",
    "        if random.random() < self.noise_prob:\n",
    "            image = self._random_noise(image)  # Apply noise only to the image, not the mask\n",
    "        return image, mask\n",
    "\n",
    "    def _random_crop(self, image, mask):\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.patch_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image_patch = image[top:top+new_h, left:left+new_w]\n",
    "        mask_patch = mask[top:top+new_h, left:left+new_w]\n",
    "\n",
    "        return image_patch, mask_patch\n",
    "\n",
    "    def _random_flip(self, image, mask):\n",
    "        return np.fliplr(image), np.fliplr(mask)\n",
    "\n",
    "    def _random_rotate(self, image, mask):\n",
    "        k = random.choice([1, 2, 3])  # 90, 180, or 270 degrees\n",
    "        return np.rot90(image, k), np.rot90(mask, k)\n",
    "\n",
    "    def _random_noise(self, image):\n",
    "        noise = np.random.normal(0, 0.05, image.shape)\n",
    "        return np.clip(image + noise, 0, 1)\n",
    "\n",
    "    def _normalize(self, image):\n",
    "        #return (image - np.mean(image)) / np.std(image)\n",
    "        normalized = np.zeros_like(image, dtype=np.float32)\n",
    "        for i in range(image.shape[2]):\n",
    "            channel = image[:,:,i]\n",
    "            mean = np.mean(channel)\n",
    "            std = np.std(channel)\n",
    "            normalized[:,:,i] = (channel - mean) / (std + 1e-8)  # adding small epsilon to avoid division by zero\n",
    "        return normalized\n",
    "\n",
    "    def prep(self, image):\n",
    "        return Tensor(self._normalize(image), requires_grad=False).permute(2,0,1).unsqueeze(0)\n",
    "\n",
    "    def split_image_into_chunks(self, image, chunk_size=64):\n",
    "        height, width, channels = image.shape\n",
    "        chunks_h = height // chunk_size\n",
    "        chunks_w = width // chunk_size\n",
    "        reshaped = image.reshape(chunks_h, chunk_size, chunks_w, chunk_size, channels)\n",
    "        return reshaped.transpose(0, 2, 1, 3, 4).reshape(-1, chunk_size, chunk_size, channels)\n",
    "        \n",
    "    def get_model_input_chunks(self, image, chunk_size=64):\n",
    "        chunks = self.split_image_into_chunks(self._normalize(image), chunk_size)\n",
    "        return chunks.transpose(0,3,1,2)\n",
    "    \n",
    "    def synthesize_image_from_chunks(self, chunks, original_shape):\n",
    "        height, width, channels = original_shape\n",
    "        chunk_size = chunks.shape[1]  # Assuming chunks are square\n",
    "        chunks_h = height // chunk_size\n",
    "        chunks_w = width // chunk_size\n",
    "        reshaped = chunks.reshape(chunks_h, chunks_w, chunk_size, chunk_size, channels)\n",
    "        transposed = reshaped.transpose(0, 2, 1, 3, 4)\n",
    "        return transposed.reshape(height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5f8ac6-975e-49b2-a85c-dac8b4524715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a3172-7267-44ac-b815-092d0eefec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    image_dir=\"data/auto_crop\",\n",
    "    mask_dir=\"data/mask\",\n",
    "    patch_size=(64,64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c3164-b8b1-40b5-b3b4-ad903c6cbd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af56f3-28ca-4591-8290-de4d86cdd514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl.normalize=False\n",
    "for a,b in zip(*dl.get_batch(8)):\n",
    "    a = a.numpy().astype(np.uint8).transpose(1,2,0)\n",
    "    b = b.numpy().astype(np.uint8) * 255\n",
    "    if np.any(b > 0):\n",
    "        display(Image.fromarray(a))\n",
    "        display(Image.fromarray(b, mode=\"L\"))\n",
    "dl.normalize=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea852d9-d1fc-4f6d-8a5c-2578887ed40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5ca1-d5ce-4050-9b26-f762c3ebb075",
   "metadata": {},
   "source": [
    "# Train UNet to extract map features from raw screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81125455-cf7f-4fea-9bdb-773240d71149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "X, Y = dl.get_batch(8)\n",
    "pred = model(X)\n",
    "s = pred.shape\n",
    "pred.permute(0,2,3,1).reshape(-1, s[1]).cross_entropy(Y.reshape(-1)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c19c5-207d-4342-ad83-9408de75fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d211d5-39a7-4216-ab5b-bb6aea29cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.Adam(nn.state.get_parameters(model))\n",
    "batch_size = 64\n",
    "def step():\n",
    "    Tensor.training = True \n",
    "    X, Y = dl.get_batch(batch_size)\n",
    "    optim.zero_grad()\n",
    "    pred = model(X)\n",
    "    s = pred.shape\n",
    "    # Need to flatten for cross_entropy to work\n",
    "    loss = pred.permute(0,2,3,1).reshape(-1, s[1]).cross_entropy(Y.reshape(-1)).backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "jit_step = TinyJit(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3eac4-00b1-43b4-a8c4-39238fe5c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(200):\n",
    "    loss = jit_step()\n",
    "    if step%5 == 0:\n",
    "        Tensor.training = False\n",
    "        X_test, Y_test = dl.get_batch(batch_size)\n",
    "        acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n",
    "        print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b325cb-b18c-4a80-9f73-2a553cda9f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373e2f0-65cb-43b3-8e40-f16e8eacd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dl.get_batch(10)\n",
    "\n",
    "y_pred = model(x).argmax(axis=1).cast(dtypes.uint8).numpy()\n",
    "y = y.cast(dtypes.uint8).numpy()\n",
    "for a,b in zip(y_pred,y):\n",
    "    #if np.any(b > 0):\n",
    "    if True:\n",
    "        display(Image.fromarray(a * 255, mode=\"L\"))\n",
    "        display(Image.fromarray(b * 255, mode=\"L\"))\n",
    "        print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c9336-9428-4f5c-9f74-9d7923b55544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec125c77-9c10-44a9-8fb8-5440d94a0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unet1\"\n",
    "#safe_save(get_state_dict(model), f\"data/model/{model_name}.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2eb88-4dad-4d95-bb6b-6c57e7df53a5",
   "metadata": {},
   "source": [
    "# Extract layouts from coast map screenshots, randomly sampled in game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e57518-a4df-417e-9ffe-4dae90482681",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor.training=False\n",
    "model = UNet()\n",
    "model_name = \"unet1\"\n",
    "state_dict = safe_load(f\"data/model/{model_name}.safetensors\")\n",
    "load_state_dict(model, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ebd68-f6d5-4eb2-9c88-5a340f3d731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_square_multiple(array, square_size):\n",
    "    h, w, c = array.shape\n",
    "    new_h = int(np.ceil(h / square_size) * square_size)\n",
    "    new_w = int(np.ceil(w / square_size) * square_size)\n",
    "    pad_h = new_h - h\n",
    "    pad_w = new_w - w\n",
    "    return np.pad(array, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "\n",
    "def run(model, x):\n",
    "    return model(x).argmax(axis=1, keepdim=True).cast(dtypes.uint8).permute(0,2,3,1).numpy()\n",
    "    \n",
    "jit_run = TinyJit(run)\n",
    "\n",
    "def batch_inference(model, chunks, chunk_size=64, batch_size=64):\n",
    "    # Inference on the whole image takes too much GPU memory, so we run inference on subsets\n",
    "    result = np.empty((0, chunk_size, chunk_size, 1), dtype=np.uint8)\n",
    "    for i in range(0, x.shape[0], batch_size):\n",
    "        model_input = Tensor(x[i:i + batch_size])\n",
    "        # TinyJit throws exception when the tensor shape changes\n",
    "        if i + batch_size <= x.shape[0]:\n",
    "            model_output = jit_run(model, model_input)\n",
    "        else:\n",
    "            model_output = run(model, model_input)\n",
    "            \n",
    "        result = np.concatenate((result, model_output), axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b754b16-beef-47ff-a920-fa0b112e471f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb8458-d8ff-4a8b-ba25-c2a95d94eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_path = \"data/layout\"\n",
    "for file in os.listdir(f\"{layout_path}/raw\"):\n",
    "    path = os.path.join(f\"{layout_path}/raw\", file)\n",
    "    x = Image.open(path)\n",
    "    x = np.array(x)[:,:,:3]\n",
    "    x = pad_to_square_multiple(x, 64)\n",
    "    original_shape = x.shape\n",
    "    chunk_size = 64\n",
    "    x = dl.get_model_input_chunks(x, chunk_size=chunk_size)\n",
    "    x = batch_inference(model, x, chunk_size=chunk_size)\n",
    "    x = dl.synthesize_image_from_chunks(x, (*original_shape[0:2], 1)).squeeze(-1)\n",
    "    output_path = os.path.join(f\"{layout_path}/extract\", os.path.splitext(file)[0] + \".npz\")\n",
    "    np.savez_compressed(output_path, data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be7c88-2447-447a-b2b9-a73efc480532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex = 2\n",
    "path = f\"data/layout/raw/coast-{ex}.png\"\n",
    "display(Image.open(path))\n",
    "output_path = f\"data/layout/extract/coast-{ex}.npz\"\n",
    "display(Image.fromarray(np.load(output_path)['data'] * 255, mode=\"L\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c376f9-209d-416d-a221-bd7eca3bc5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdd0c623-7db3-4954-a6ca-c1239b9a8e13",
   "metadata": {},
   "source": [
    "# Manually label layout classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e637fd3-7436-4527-a1ef-97ef8654e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_square(image, max_size):\n",
    "    image = image.astype(np.uint8)\n",
    "    height, width = image.shape\n",
    "    scale = min(max_size / width, max_size / height)\n",
    "    new_width = int(width * scale)\n",
    "    new_height = int(height * scale)\n",
    "    block_height = height // new_height\n",
    "    block_width = width // new_width\n",
    "    reshaped = image[:block_height*new_height, :block_width*new_width]\n",
    "    reshaped = reshaped.reshape(new_height, block_height, new_width, block_width)\n",
    "    pooled = reshaped.max(axis=(1, 3))\n",
    "    square_image = np.zeros((max_size, max_size), dtype=np.uint8)\n",
    "    pad_y = (max_size - new_height) // 2\n",
    "    pad_x = (max_size - new_width) // 2\n",
    "    square_image[pad_y:pad_y+new_height, pad_x:pad_x+new_width] = pooled\n",
    "    return square_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13b5b4-3cf7-4f17-a24f-1372b1a96cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_labels = [\n",
    "    0, 1, 0, 2,\n",
    "    3, 2, 4, 5,\n",
    "    6, 6, 7, 3,\n",
    "    8, 7,\n",
    "]\n",
    "layouts = []\n",
    "layout_dir = \"data/layout/extract\"\n",
    "for file in sorted(os.listdir(layout_dir), key=lambda x: int(x.split('coast-')[1].split('.npz')[0])):\n",
    "    layout = np.load(os.path.join(layout_dir, file))['data']\n",
    "    layouts.append(resize_square(layout, 400))\n",
    "\n",
    "row_size = 4\n",
    "for i in range(0, len(layouts), row_size):\n",
    "    row = np.concatenate(layouts[i:i + row_size], axis=1)\n",
    "    display(Image.fromarray(row * 255, mode=\"L\"))\n",
    "    labels = truth_labels[i:i+row_size]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dfdbf2-fa42-42f2-beeb-dfa977e0f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_labels = [\n",
    "    0, 1, 0, 2,\n",
    "    3, 2, 4, 5,\n",
    "    6, 6, 7, 3,\n",
    "    8, 7,\n",
    "]\n",
    "layouts = []\n",
    "layout_dir = \"data/layout/extract\"\n",
    "for file in sorted(os.listdir(layout_dir), key=lambda x: int(x.split('coast-')[1].split('.npz')[0])):\n",
    "    layout = np.load(os.path.join(layout_dir, file))['data']\n",
    "    layouts.append(resize_square(layout, 400))\n",
    "\n",
    "grouped = {}\n",
    "for i, label in enumerate(truth_labels):\n",
    "    if grouped.get(label) is None:\n",
    "        grouped[label] = []\n",
    "    grouped[label].append(layouts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60689bce-f201-4ca6-93d8-d5ba2d186f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_layouts, unique_labels = [], []\n",
    "for group in grouped:\n",
    "    unique_layouts.append(grouped[group][0])\n",
    "    unique_labels.append(group)\n",
    "                          \n",
    "row_size = 4\n",
    "for i in range(0, len(unique_layouts), row_size):\n",
    "    row = np.concatenate(unique_layouts[i:i + row_size], axis=1)\n",
    "    display(Image.fromarray(row * 255, mode=\"L\"))\n",
    "    labels = unique_labels[i:i+row_size]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0a8aa-94c9-44c9-a944-cdae4349a5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d0bb5-33a1-47b4-8f26-e23ef09fb410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_icon_mask(image, icon, original_mask, threshold=0.98):\n",
    "    # Perform template matching\n",
    "    result = cv2.matchTemplate(image, icon, cv2.TM_CCOEFF_NORMED)\n",
    "    \n",
    "    # Find all locations where the matching exceeds the threshold\n",
    "    locations = np.where(result >= threshold)\n",
    "    locations = list(zip(*locations[::-1]))  # Reverse to get (x, y)\n",
    "    \n",
    "    # Create a new mask for the icons\n",
    "    icon_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    h, w = icon.shape[:2]\n",
    "\n",
    "    ############### keep only left-most icon\n",
    "    ##### this is a hack based on coast layout\n",
    "    locations = [min(locations, key=lambda x: x[0])]\n",
    "    \n",
    "    # Draw all icon locations on the mask\n",
    "    for loc in locations:\n",
    "        print(loc)\n",
    "        top_left = loc\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "        icon_mask[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]] = 1\n",
    "    \n",
    "    # Create the new mask with two channels\n",
    "    new_mask = np.zeros((*original_mask.shape, 2), dtype=original_mask.dtype)\n",
    "    new_mask[:,:,0] = original_mask\n",
    "    new_mask[:,:,1] = icon_mask\n",
    "    \n",
    "    return new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559fadca-32c0-4a5f-9a48-482c2bb54175",
   "metadata": {},
   "outputs": [],
   "source": [
    "door = np.load(\"data/door.npz\")['data']\n",
    "image = pad_to_square_multiple(np.array(Image.open(\"data/layout/raw/coast-0.png\"))[:,:,0:3], 64)\n",
    "original_mask = np.load(\"data/layout/extract/coast-0.npz\")['data']\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9cdc5-c885-4dff-a3b7-52bbdb4e4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = create_icon_mask(image, door, original_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ee4b8-37b4-4703-87cc-bb76d01beb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb443b63-d8fe-4890-9aa1-8b526619f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsdf = (asdf[:,:,0] + asdf[:,:,1]).reshape(asdf.shape[0:2])\n",
    "Image.fromarray(bsdf * 255, mode=\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db421da0-5740-4d45-aeed-81ca18d70f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poemap",
   "language": "python",
   "name": "poemap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
