{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4568c2-1b19-4038-a121-9273b86a4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from:\n",
    "https://github.com/milesial/Pytorch-UNet\n",
    "https://github.com/tinygrad/tinygrad/examples/stable_diffusion.py\n",
    "https://docs.tinygrad.org/mnist/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994a7f9-78fc-488f-853e-57392a1f0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, TinyJit, nn\n",
    "from tinygrad.nn import Conv2d, ConvTranspose2d, BatchNorm2d\n",
    "from tinygrad.dtype import dtypes\n",
    "from tinygrad.nn.state import safe_load, safe_save, get_state_dict, load_state_dict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837a9d3-b471-453b-b13b-cc92f03de798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleconv(in_chan, out_chan):\n",
    "    return [Conv2d(in_chan, out_chan, kernel_size=3, padding=1), BatchNorm2d(out_chan), Tensor.relu,\n",
    "        Conv2d(out_chan, out_chan, kernel_size=3, padding=1), BatchNorm2d(out_chan), Tensor.relu]\n",
    "\n",
    "class UNet:\n",
    "    def __init__(self):\n",
    "        self.save_intermediates = [\n",
    "            doubleconv(3, 64), \n",
    "            [Tensor.max_pool2d, *doubleconv(64, 128)],\n",
    "        ]\n",
    "        self.middle = [\n",
    "            Tensor.max_pool2d, *doubleconv(128, 256),\n",
    "            ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "        ]\n",
    "        self.consume_intermediates = [\n",
    "            [*doubleconv(256, 128), ConvTranspose2d(128, 64, kernel_size=2, stride=2)],\n",
    "            [*doubleconv(128, 64), Conv2d(64, 2, kernel_size=1)],\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        intermediates = []\n",
    "        for b in self.save_intermediates:\n",
    "            for bb in b:\n",
    "                x = bb(x)\n",
    "            intermediates.append(x)\n",
    "        for bb in self.middle:\n",
    "            x = bb(x)\n",
    "        for b in self.consume_intermediates:\n",
    "            x = intermediates.pop().cat(x, dim=1)\n",
    "            for bb in b:\n",
    "                x = bb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64663adb-435b-491a-ba16-e27389c0ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMaskPair:\n",
    "    def __init__(self, image_path, mask_path):\n",
    "        self.image_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "\n",
    "    def load_image(self):\n",
    "        return np.load(self.image_path)['data']\n",
    "\n",
    "    def load_mask(self):\n",
    "        return np.load(self.mask_path)['data']\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, image_dir, mask_dir, patch_size=(64, 64), normalize=True, \n",
    "                 flip_prob=0.5, rotate_prob=0.5, noise_prob=0,):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.normalize = normalize\n",
    "        self.flip_prob = flip_prob\n",
    "        self.rotate_prob = rotate_prob\n",
    "        self.noise_prob = noise_prob\n",
    "        self.image_mask_pairs = self.get_image_mask_pairs()\n",
    "\n",
    "    def get_image_mask_pairs(self):\n",
    "        ret = []\n",
    "        for subdir in os.listdir(self.image_dir):\n",
    "            for file in os.listdir(os.path.join(self.image_dir, subdir)):\n",
    "                im_file = os.path.join(self.image_dir, subdir, file)\n",
    "                mask_file = os.path.join(self.mask_dir, subdir + \".npz\")\n",
    "                ret.append(ImageMaskPair(im_file, mask_file))\n",
    "\n",
    "        ret = sorted(ret, key = lambda x: x.mask_path)\n",
    "        return ret\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # Randomly distribute samples across images\n",
    "        shares = np.random.dirichlet(np.ones(len(self.image_mask_pairs)), size=1)[0]\n",
    "        result = np.round(shares * batch_size).astype(int)\n",
    "        # Adjust to ensure sum is exactly batch_size\n",
    "        diff = batch_size - result.sum()\n",
    "        result[np.argmax(result)] += diff\n",
    "\n",
    "        image_patches, mask_patches = [], []\n",
    "        mask_cache = {}\n",
    "        for i, num_samples in enumerate(result):\n",
    "            imp = self.image_mask_pairs[i]\n",
    "            if mask_cache.get(imp.mask_path) is None:\n",
    "                # We sorted image_mask_pairs by mask, so we don't need to cache previous masks\n",
    "                mask_cache = {imp.mask_path: imp.load_mask()}\n",
    "            mask = mask_cache[imp.mask_path]\n",
    "            image = imp.load_image()\n",
    "            image = self._normalize(image) if self.normalize else image\n",
    "            for _ in range(num_samples):\n",
    "                ip, mp = self._random_crop(image, mask)\n",
    "                ip, mp = self._apply_augmentations(ip, mp)\n",
    "                image_patches.append(ip)\n",
    "                mask_patches.append(mp)\n",
    "        image_patches = Tensor(image_patches).permute(0,3,1,2)\n",
    "        return image_patches, Tensor(mask_patches)\n",
    "\n",
    "    def _apply_augmentations(self, image, mask):\n",
    "        if random.random() < self.flip_prob:\n",
    "            image, mask = self._random_flip(image, mask)\n",
    "        if random.random() < self.rotate_prob:\n",
    "            image, mask = self._random_rotate(image, mask)\n",
    "        if random.random() < self.noise_prob:\n",
    "            image = self._random_noise(image)  # Apply noise only to the image, not the mask\n",
    "        return image, mask\n",
    "\n",
    "    def _random_crop(self, image, mask):\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.patch_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image_patch = image[top:top+new_h, left:left+new_w]\n",
    "        mask_patch = mask[top:top+new_h, left:left+new_w]\n",
    "\n",
    "        return image_patch, mask_patch\n",
    "\n",
    "    def _random_flip(self, image, mask):\n",
    "        return np.fliplr(image), np.fliplr(mask)\n",
    "\n",
    "    def _random_rotate(self, image, mask):\n",
    "        k = random.choice([1, 2, 3])  # 90, 180, or 270 degrees\n",
    "        return np.rot90(image, k), np.rot90(mask, k)\n",
    "\n",
    "    def _random_noise(self, image):\n",
    "        noise = np.random.normal(0, 0.05, image.shape)\n",
    "        return np.clip(image + noise, 0, 1)\n",
    "\n",
    "    def _normalize(self, image):\n",
    "        #return (image - np.mean(image)) / np.std(image)\n",
    "        normalized = np.zeros_like(image, dtype=np.float32)\n",
    "        for i in range(image.shape[2]):\n",
    "            channel = image[:,:,i]\n",
    "            mean = np.mean(channel)\n",
    "            std = np.std(channel)\n",
    "            normalized[:,:,i] = (channel - mean) / (std + 1e-8)  # adding small epsilon to avoid division by zero\n",
    "        return normalized\n",
    "\n",
    "    def prep(self, image):\n",
    "        return Tensor(self._normalize(image)).permute(2,0,1).unsqueeze(0)\n",
    "\n",
    "    def split_image_into_chunks(self, image, chunk_size=64):\n",
    "        height, width, channels = image.shape\n",
    "        chunks_h = height // chunk_size\n",
    "        chunks_w = width // chunk_size\n",
    "        reshaped = image.reshape(chunks_h, chunk_size, chunks_w, chunk_size, channels)\n",
    "        return reshaped.transpose(0, 2, 1, 3, 4).reshape(-1, chunk_size, chunk_size, channels)\n",
    "        \n",
    "    def image_to_model_input(self, image, chunk_size=64):\n",
    "        chunks = self.split_image_into_chunks(self._normalize(image), chunk_size)\n",
    "        return Tensor(chunks).permute(0,3,1,2)\n",
    "    \n",
    "    def synthesize_image_from_chunks(self, chunks, original_shape):\n",
    "        height, width, channels = original_shape\n",
    "        chunk_size = chunks.shape[1]  # Assuming chunks are square\n",
    "        chunks_h = height // chunk_size\n",
    "        chunks_w = width // chunk_size\n",
    "        reshaped = chunks.reshape(chunks_h, chunks_w, chunk_size, chunk_size, channels)\n",
    "        transposed = reshaped.transpose(0, 2, 1, 3, 4)\n",
    "        return transposed.reshape(height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1daac-c577-417d-add3-b41ea5efb0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a3172-7267-44ac-b815-092d0eefec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    image_dir=\"data/auto_crop\",\n",
    "    mask_dir=\"data/mask\",\n",
    "    patch_size=(64,64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c3164-b8b1-40b5-b3b4-ad903c6cbd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af56f3-28ca-4591-8290-de4d86cdd514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.normalize=False\n",
    "for a,b in zip(*dl.get_batch(8)):\n",
    "    a = a.numpy().astype(np.uint8).transpose(1,2,0)\n",
    "    b = b.numpy().astype(np.uint8) * 255\n",
    "    if np.any(b > 0):\n",
    "        display(Image.fromarray(a))\n",
    "        display(Image.fromarray(b, mode=\"L\"))\n",
    "dl.normalize=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea852d9-d1fc-4f6d-8a5c-2578887ed40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81125455-cf7f-4fea-9bdb-773240d71149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "X, Y = dl.get_batch(8)\n",
    "pred = model(X)\n",
    "s = pred.shape\n",
    "pred.permute(0,2,3,1).reshape(-1, s[1]).cross_entropy(Y.reshape(-1)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c19c5-207d-4342-ad83-9408de75fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d211d5-39a7-4216-ab5b-bb6aea29cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.Adam(nn.state.get_parameters(model))\n",
    "batch_size = 128\n",
    "def step():\n",
    "    Tensor.training = True \n",
    "    X, Y = dl.get_batch(batch_size)\n",
    "    optim.zero_grad()\n",
    "    pred = model(X)\n",
    "    s = pred.shape\n",
    "    # Need to flatten for cross_entropy to work\n",
    "    loss = pred.permute(0,2,3,1).reshape(-1, s[1]).cross_entropy(Y.reshape(-1)).backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "jit_step = TinyJit(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3eac4-00b1-43b4-a8c4-39238fe5c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(1000):\n",
    "    loss = jit_step()\n",
    "    if step%10 == 0:\n",
    "        Tensor.training = False\n",
    "        X_test, Y_test = dl.get_batch(batch_size)\n",
    "        acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n",
    "        print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b325cb-b18c-4a80-9f73-2a553cda9f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373e2f0-65cb-43b3-8e40-f16e8eacd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dl.get_batch(10)\n",
    "\n",
    "y_pred = model(x).argmax(axis=1).cast(dtypes.uint8).numpy()\n",
    "y = y.cast(dtypes.uint8).numpy()\n",
    "for a,b in zip(y_pred,y):\n",
    "    #if np.any(b > 0):\n",
    "    if True:\n",
    "        display(Image.fromarray(a * 255, mode=\"L\"))\n",
    "        display(Image.fromarray(b * 255, mode=\"L\"))\n",
    "        print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c9336-9428-4f5c-9f74-9d7923b55544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec125c77-9c10-44a9-8fb8-5440d94a0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unet\"\n",
    "safe_save(get_state_dict(model), f\"data/model/{model_name}.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e57518-a4df-417e-9ffe-4dae90482681",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unet\"\n",
    "state_dict = safe_load(f\"data/model/{model_name}.safetensors\")\n",
    "load_state_dict(model, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974465bd-3bc4-402d-b687-d03d38d50a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ebd68-f6d5-4eb2-9c88-5a340f3d731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_square_multiple(array, square_size):\n",
    "    h, w, c = array.shape\n",
    "    new_h = int(np.ceil(h / square_size) * square_size)\n",
    "    new_w = int(np.ceil(w / square_size) * square_size)\n",
    "    pad_h = new_h - h\n",
    "    pad_w = new_w - w\n",
    "    return np.pad(array, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d086d0-a01c-40b3-a094-ff9321999f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = Image.fromarray(np.load(\"data/mask/0.npz\")['data'] * 255, mode=\"L\")\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436ef15-023f-4ddf-a3b7-219a37bcf257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fa687-2cfc-4759-83a2-c23f65926691",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"data/auto_crop/0/3.npz\")['data']\n",
    "#x = Image.open(\"data/layout/raw/coast-1.png\")\n",
    "x = np.array(x)[:,:,:3]\n",
    "padded_image = pad_to_square_multiple(x, 64)\n",
    "display(Image.fromarray(padded_image))\n",
    "x = dl.prep(padded_image)\n",
    "y = model(x).argmax(axis=1).cast(dtypes.uint8).numpy().squeeze(0)\n",
    "display(Image.fromarray(y * 255, mode=\"L\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11965d4-3d25-4f9b-8ca4-3e619b59c0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57e504-b9a8-4b02-9f63-d0f3d56a5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pad_to_square_multiple(np.load(\"data/auto_crop/0/3.npz\")['data'], 64)\n",
    "#x = Image.open(\"data/layout/raw/coast-9ex.png\")\n",
    "x = pad_to_square_multiple(np.array(x)[:,:,:3], 64)\n",
    "original_shape = x.shape\n",
    "x = dl.image_to_model_input(x, chunk_size=64)\n",
    "y = model(x).argmax(axis=1, keepdim=True).cast(dtypes.uint8).permute(0,2,3,1).numpy()\n",
    "y = dl.synthesize_image_from_chunks(y, (*original_shape[0:2], 1)).squeeze(-1)\n",
    "display(Image.fromarray(y * 255, mode=\"L\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5d0e9-f073-4cf3-aa8c-e58633542348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poemap",
   "language": "python",
   "name": "poemap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
