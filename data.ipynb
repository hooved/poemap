{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a6a33-03bb-4bbe-87c9-432aa7dc3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697f84d-12b4-4bbd-8dd7-f851e4f2c94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53123d-f1a2-4a76-825d-c6b487d5bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want to train a model to take any game screenshot and extract only the map features.\n",
    "This is what people do when they look at the screen.\n",
    "\n",
    "To assemble training data, we start by taking a screenshot in game, manually crop the image \n",
    "with MS paint to a smaller area that only contains the map information, with minimal stuff \n",
    "in the background.\n",
    "\n",
    "The blue map lines seem like their colors can be distored by background interference \n",
    "(e.g. fires), which we minimize with this manual crop. Then we can just use color filters to\n",
    "extract the map lines.\n",
    "\n",
    "These extracted map lines are ground truth. The other screenshots in our model training data\n",
    "have the same lines, but overlayed on much noisier backgrounds. The model will learn to \n",
    "convert the noisy screenshots to the clean map features.\n",
    "\"\"\"\n",
    "manual_crop = Image.open(\"data/manual_crop/0.png\")\n",
    "# remove alpha channel added while cropping with ms paint\n",
    "display(manual_crop)\n",
    "manual_crop = np.array(manual_crop)[:,:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2265db0-37ab-49df-bcd1-aeef062bf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_color_ranges(image, ranges):\n",
    "    \"\"\"\n",
    "    Filter an image to keep only pixels within specified color ranges.\n",
    "    \n",
    "    :param image: NumPy array of shape (height, width, 3) with values 0-255\n",
    "    :param ranges: List of tuples, each containing min and max values for R, G, and B\n",
    "    :return: Filtered image with pixels outside the ranges set to black\n",
    "    \"\"\"\n",
    "    # Create a copy of the image\n",
    "    filtered_image = image.copy()\n",
    "    \n",
    "    # Create a mask initialized with all False\n",
    "    mask = np.zeros(image.shape[:2], dtype=bool)\n",
    "    \n",
    "    # For each color range, update the mask\n",
    "    for (r_min, r_max), (g_min, g_max), (b_min, b_max) in ranges:\n",
    "        range_mask = (\n",
    "            (image[:,:,0] >= r_min) & (image[:,:,0] <= r_max) &\n",
    "            (image[:,:,1] >= g_min) & (image[:,:,1] <= g_max) &\n",
    "            (image[:,:,2] >= b_min) & (image[:,:,2] <= b_max)\n",
    "        )\n",
    "        mask |= range_mask\n",
    "    \n",
    "    # Apply the mask: set pixels outside the ranges to black\n",
    "    filtered_image[~mask] = [0, 0, 0]\n",
    "    \n",
    "    return filtered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08aad65-353f-4d70-ab9a-d6fd58188294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color ranges: (R_min, R_max), (G_min, G_max), (B_min, B_max)\n",
    "bluish_range = ((110, 150), (0, 150), (150, 255))\n",
    "\n",
    "color_ranges = [bluish_range]\n",
    "\n",
    "map_lines = filter_color_ranges(manual_crop, color_ranges)\n",
    "display(Image.fromarray(map_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc728f-2496-47a0-a450-5a9e0e9df508",
   "metadata": {},
   "outputs": [],
   "source": [
    "orangeish_range = ((180, 220), (80, 125), (35, 45))  # More red, some green, little blue\n",
    "color_ranges = [orangeish_range]\n",
    "\n",
    "entrance = filter_color_ranges(gt_source, color_ranges)\n",
    "display(Image.fromarray(entrance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8f455-612e-4e72-a3df-d12c3efd129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge images\n",
    "# Create a mask of non-black pixels in the overlay image\n",
    "mask = np.any(entrance != [0, 0, 0], axis=-1)\n",
    "# Create a copy of the base image\n",
    "merge = map_lines.copy()\n",
    "# Replace pixels in the result where the mask is True\n",
    "merge[mask] = entrance[mask]\n",
    "\n",
    "Image.fromarray(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8db60b-a409-4acc-aabf-0505d0732145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Current approach is to train a model to extract features in above image, given in-game image.\n",
    " Our simple color-based filtering worked here, but will not work most of the time,\n",
    "given diverse lighting conditions, terrain, character graphics, etc. which will interfere with\n",
    "the filters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5221a-a388-44d0-b32d-078153e4012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_icon(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Threshold the image to create a binary mask\n",
    "    _, binary = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Find the bounding rectangle of the icon\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "    \n",
    "    # Crop the icon\n",
    "    icon = img[y:y+h, x:x+w]\n",
    "    \n",
    "    return icon\n",
    "\n",
    "door = extract_icon(entrance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7402b-ccb4-477a-8454-9828e42ae9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.all(door != 0, axis=-1)\n",
    "def largest_true_subsection(arr):\n",
    "    if not arr.any():\n",
    "        return (0, 0, 0, 0)  # No True values\n",
    "    \n",
    "    rows, cols = arr.shape\n",
    "    heights = np.zeros(cols, dtype=int)\n",
    "    max_area = 0\n",
    "    max_rect = (0, 0, 0, 0)  # (top, left, height, width)\n",
    "\n",
    "    for row in range(rows):\n",
    "        heights = (heights + 1) * arr[row]\n",
    "        stack = [-1]\n",
    "        \n",
    "        for col in range(cols + 1):\n",
    "            h = heights[col] if col < cols else 0\n",
    "            while stack[-1] != -1 and heights[stack[-1]] > h:\n",
    "                height = heights[stack.pop()]\n",
    "                width = col - stack[-1] - 1\n",
    "                area = height * width\n",
    "                if area > max_area:\n",
    "                    max_area = area\n",
    "                    max_rect = (row - height + 1, stack[-1] + 1, height, width)\n",
    "            stack.append(col)\n",
    "\n",
    "    return max_rect\n",
    "\n",
    "subsection = largest_true_subsection(mask)\n",
    "\n",
    "def extract_subsection(arr, subsection):\n",
    "    top, left, height, width = subsection\n",
    "    return arr[top:top+height, left:left+width]\n",
    "    \n",
    "door_subsection = extract_subsection(door, subsection)\n",
    "Image.fromarray(door_subsection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cff669-e1dd-435e-b788-87f0c0809072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RGB to BGR for OpenCV processing\n",
    "manual_crop_bgr = cv2.cvtColor(manual_crop, cv2.COLOR_RGB2BGR)\n",
    "door_subsection_bgr = cv2.cvtColor(door_subsection, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58cc87-e197-4284-a81a-ed8682441bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46dce0-1141-43c9-8b17-097d8cbab1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically crop the source image to the same size as the manually cropped image,\n",
    "# with perfect overlap of the door subsection between the two images\n",
    "image = np.array(Image.open(\"data/screenshot/0/0.png\"))\n",
    "image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "method = cv2.TM_CCOEFF_NORMED\n",
    "res1 = cv2.matchTemplate(manual_crop_bgr, door_subsection_bgr, method)\n",
    "res2 = cv2.matchTemplate(image_bgr, door_subsection_bgr, method)\n",
    "\n",
    "_, _, _, max_loc1 = cv2.minMaxLoc(res1)\n",
    "_, _, _, max_loc2 = cv2.minMaxLoc(res2)\n",
    "\n",
    "shift_x = max_loc2[0] - max_loc1[0]\n",
    "shift_y = max_loc2[1] - max_loc1[1]\n",
    "\n",
    "image1 = image\n",
    "image2 = manual_crop\n",
    "\n",
    "# Get dimensions\n",
    "h1, w1 = image.shape[:2]\n",
    "h2, w2 = gt.shape[:2]\n",
    "\n",
    "# Calculate the crop region for image1\n",
    "x1 = max(0, shift_x)\n",
    "y1 = max(0, shift_y)\n",
    "x2 = min(w1, w2 + shift_x)\n",
    "y2 = min(h1, h2 + shift_y)\n",
    "\n",
    "cropped_overlap = image[y1:y2, x1:x2]\n",
    "display(Image.fromarray(cropped_overlap))\n",
    "display(Image.fromarray(manual_crop))\n",
    "diff = cropped_overlap - manual_crop\n",
    "assert np.all(diff == 0), \"not a pixel-perfect alignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a399d7-9373-4bdd-a18e-add7f65e1f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poemap",
   "language": "python",
   "name": "poemap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
